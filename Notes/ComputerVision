Computer Vision Problems
==========================
- Image Classification
- Object Detection
- Neural Style Transfer

4 Computer Vision problems:
- Semantic Segmentation
- Classification+Localization
- Object Detection
- Instance Segmentation

Why Convolutions:
===================
Example we have (1000, 1000, 3) input and 1000 hidden layer units.
Then Parameters to learn are 3 Billion which is just very, very large.
Problems:
- Overfitting cant be prevented
- High Computational and memory requirements to train a neural network

- Sparcity of Connections
 - Each activation in next layer depends on only a small no of activations from prev layer
- Parameter Sharing
 - reduces total no of parameters, thus reduces overfitting
 - Allows feature detector to be used in multiple locations throughout whole input image

Padding
=========
Valid - No Padding (p=0)
Same - Paddding so that Output Size and Input size are same (p=(no of filters-1)/2)


Output Size formula of Convolution Layer:
==========================================
n * n image
f * f  filter
padding p
stride s
Floor [ (n+2p-f)/s + 1 ] * Floor [ (n+2p-f)/s + 1 ]

for Padding Valid , p=0
for Padding SAME , p = f-1/2

No of paramteres in one layer:
===============================
10 filters
Each filter 3*3*3
# parameter = (3*3*3(Weights) + 1(Bias)) * 10(filters) = 280

Summary of Notation
====================
f[l] = filter size
p[l] = padding
s[l] = stride
nc[l] = # of filters
nc[l-1] = # of channels. In case of RGB its 3 and GreyScale its 1
Input = nh[l-1]*nw[l-1]*nc[l-1]
Output = nh[l]*nw[l]*nc[l]
Each Filter is of size = f[l]*f[l]*nc[l-1]
Activation a[l] = nh[l] * nw[l] * nc[l]
Weights = f[l] * f[l] * nc[l-1] * nc[l]
Bias = 1*1*1*nc[l]
A[l] = m*nh[l] * nw[l] * nc[l]

Types of Layer in ConvNet
===========================
- Convolution (Conv)
- Pooling (Pool)
- Fully Connected (FC)

Pooling Layer
==============
zero parameter to learn
Hyper parameters
 - filter size
 - stride
two types
 - Max Pooling
 - Average Pooling

Classic NN
===========
LeNEt-5 (Input-> Conv->Pool->Conv->Pool->FC->FC->Output)
AlexNet
VGG-16 (16 Layers having weights , ~138M parameters)
ResNet (Shortcut / Skip Connection)

Very, very deep neural networks are difficult to train because of
vanishing and exploding gradient types of problems

1*1 Convolution can decrease # of channels.
Inception Module(GoogleNet) uses 1 by 1 conv

Image Filtering tutorial
https://lodev.org/cgtutor/filtering.html


Libraries used for Image data:
- OpenCV
- Pillow

Data Augmentation
https://keras.io/preprocessing/image/
Transfer Learning
https://www.tensorflow.org/tutorials/images/transfer_learning

Rock-Paper-Scissors Dataset from Laurence
http://www.laurencemoroney.com/rock-paper-scissors-dataset/

To avoid overfitting:

1. Data Augmentation
These are just a few of the options available (for more, see the Keras documentation.
rotation_range is a value in degrees (0â€“180), a range within which to randomly rotate pictures.
width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.
shear_range is for randomly applying shearing transformations.
zoom_range is for randomly zooming inside pictures.
horizontal_flip is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).
fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.

2. Dropouts
The idea behind Dropouts is that they remove a random number of neurons in your neural network.
This works very well for two reasons:
The first is that neighboring neurons often end up with similar weights, which can lead to overfitting,
so dropping some out at random can remove this.
The second is that often a neuron can over-weigh the input from a neuron in the previous layer,
and can over specialize as a result. Thus, dropping out can break the neural network out of this potential
bad habit!
Andrew Ng video on dropout
https://www.youtube.com/watch?v=ARq74QuavAo