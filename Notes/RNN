Types of Deep Learning
==========================
Supervised Deep Learning
 - ANN used for Regression and Classification
 - CNN used for Computer Vision
 - RNN used for Time Series Analysis
Usupervised Deep Learning
 - Self Organizing Maps used for Feature Detection
 - Deep Boltzmann Machines used for Recommnder Systems
 - Auto Encoders used for Recommnder Systems

Neural Networks and Human Brain
================================
Whole concept behind deep learning is to try to mimic the human brain and leverage the things that evolution has already developed for us.

Human Brain got 3 parts
- Cerebrum
    - Temporal Lobe (Long Term Memory) linked to ANN, Weights represent Long-term memory
    - Frontal Lobe - linked RNN for short term memory
    - Ocipital Lobe - linked to CNN
    - Parietal Lobe - Research going on for linking NN
- Cerebellum
- Brainstem

Why Not Feedforward Networks
=============================
- I cannot predict the next word in a sentence if I use it


Types of RNN
=============
One to One
One to Many - Music Generation, Image Captioning
Many to One - Sentiment Analysis/Classification
Many to Many - (Input and Output are not same) Machine Translation
Many to Many - (Input and Output are same) Named Entity Recognition

Applications
- speech recognition,
- language modeling,
- translation,
- image captioning

People Behind Recurrent Neural Networks
========================================
Sepp (Joseph) Hochreiter - vanishing gradient was first discovered in 1991
Yoshua Bengio - vanishing gradient was again discovered in 1994

Backpropagation through Time (BTT)
===================================
RNN uses back propagation algorithm , but it is applied for every timestamp. It is called BTT
Problems with it are:
- Vanishing Gradient Problem
- Exploding Gradient Problem

Solutions to the Problems
=============================================
In case of exploding gradient, you can:
- stop backpropagating after a certain point, which is usually not optimal because not all of the weights get updated; truncated BTT
- penalize or artificially reduce gradient;
- put a maximum limit on a gradient, and clip the gradients when it goes higher then threshold
- RMSprop to adjust learning rate

In case of vanishing gradient, you can:
- initialize weights so that the potential for vanishing gradient is minimized;
- have Echo State Networks that are designed to solve the vanishing gradient problem;
- have Long Short-Term Memory Networks (LSTMs).

LSTM (Long Short Term Memory) Networks
========================================
- Repeating module in a RNN contains a single layer
- Capable of learning long term dependencies
- steps in LSTM
 1. forget gate layer - decision to throw away not required information from cell state by a sigmoid layer
 2. input gate layer - add new information to the cell gate. this layer contains sigmoid and tanh layers
 3. output layer - combine above 2 steps and send the output
    - sigmoid layer which decides what parts of the cell state we’re going to output
    - we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.

